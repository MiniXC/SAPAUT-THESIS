@misc{synnaeve2020endtoend,
      title={End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern Architectures}, 
      author={G. Synnaeve and Q. Xu and J. Kahn and T. Likhomanenko and E. Grave and V. Pratap and A. Sriram and V. Liptchinsky and R. Collobert},
      year={2020},
      eprint={1911.08460},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@INPROCEEDINGS{gravano2009,  author={A. {Gravano} and M. {Jansche} and M. {Bacchiani}},  booktitle={2009 IEEE International Conference on Acoustics, Speech and Signal Processing},   title={Restoring punctuation and capitalization in transcribed speech},   year={2009},  volume={},  number={},  pages={4741-4744},}

@inproceedings{lita2003,
author = {L. V. Lita and A. Ittycheriah and S. Roukos and N. Kambhatla},
title = {TRuEcasIng},
year = {2003},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1075096.1075116},
doi = {10.3115/1075096.1075116},
abstract = {Truecasing is the process of restoring case information to badly-cased or non-cased text. This paper explores truecasing issues and proposes a statistical, language modeling based truecaser which achieves an accuracy of ~98% on news articles. Task based evaluation shows a 26% F-measure improvement in named entity recognition when using truecasing. In the context of automatic content extraction, mention detection on automatic speech recognition text is also improved by a factor of 8. Truecasing also enhances machine translation output legibility and yields a BLEU score improvement of 80.2%. This paper argues for the use of truecasing as a valuable component in text processing applications.},
booktitle = {Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1},
pages = {152–159},
numpages = {8},
location = {Sapporo, Japan},
series = {ACL '03}
}

@misc{li2020dice,
      title={Dice Loss for Data-imbalanced NLP Tasks}, 
      author={X. Li and X. Sun and Y. Meng and J. Liang and F. Wu and J. Li},
      year={2020},
      eprint={1911.02855},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{sennrich2016neural,
      title={Neural Machine Translation of Rare Words with Subword Units}, 
      author={R. Sennrich and B. Haddow and A. Birch},
      year={2016},
      eprint={1508.07909},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{yi2019,
  title={Focal Loss for Punctuation Prediction},
  author={J. Yi and J. Tao and Z. Tian and Y. Bai and C. Fan}
}

@INPROCEEDINGS{beeferman1998,

  author={D. {Beeferman} and A. {Berger} and J. {Lafferty}},

  booktitle={Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)}, 

  title={Cyberpunc: a lightweight punctuation annotation system for speech}, 

  year={1998},

  volume={2},

  number={},

  pages={689-692 vol.2},

  doi={10.1109/ICASSP.1998.675358}}


@inproceedings{Chen1999,
  title={Speech recognition with automatic punctuation},
  author={C. Chen},
  booktitle={EUROSPEECH},
  year={1999}
}

@article{christensen2001,
  title={Punctuation annotation using statistical prosody models.},
  author={H. Christensen and Y. Gotoh and S. Renals},
  year={2001}
}

@inproceedings{briscoe2002,
    title = "Robust Accurate Statistical Annotation of General Text",
    author = "T. Briscoe  and
      J. Carroll",
    booktitle = "Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02)",
    month = may,
    year = "2002",
    address = "Las Palmas, Canary Islands - Spain",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2002/pdf/250.pdf",
}

@inproceedings{Huang2002,
  title={Maximum entropy model for punctuation annotation from speech},
  author={J. Huang and G. Zweig},
  booktitle={INTERSPEECH},
  year={2002}
}

@article{kim2003,
title = "A combined punctuation generation and speech recognition system and its performance enhancement using prosody",
journal = "Speech Communication",
volume = "41",
number = "4",
pages = "563 - 577",
year = "2003",
issn = "0167-6393",
doi = "https://doi.org/10.1016/S0167-6393(03)00049-9",
url = "http://www.sciencedirect.com/science/article/pii/S0167639303000499",
author = "J. H. Kim and P. C. Woodland",
keywords = "Punctuation generation, Speech recognition, Prosody, Classification And Regression Tree (CART), -best rescoring",
abstract = "A punctuation generation system which combines prosodic information with acoustic and language model information is presented. Experiments have been conducted for both the reference text transcriptions and speech recogniser outputs. For the reference transcription, prosodic information of acoustic data is shown to be more useful than language model information. Several straightforward modifications of a conventional speech recogniser allow the system to produce punctuation and speech recognition hypotheses simultaneously. The multiple hypotheses produced by the automatic speech recogniser are then re-scored using prosodic information. When the prosodic information is incorporated, the F-measure (defined as harmonic mean of recall and precision) can be improved. This speech recognition system including punctuation gives a small reduction in word error rate on the 1-best speech recognition output including punctuation. An alternative approach for generating punctuation from the un-punctuated 1-best speech recognition output is also proposed. The results from these two alternative schemes are compared."
}

@article{batista2008,
  title={Recovering capitalization and punctuation marks for automatic speech recognition: Case study for Portuguese broadcast news},
  author={F. Batista and D. Caseiro and N. Mamede and I. Trancoso},
  journal={Speech Communication},
  volume={50},
  number={10},
  pages={847--862},
  year={2008},
  publisher={Elsevier}
}

@inproceedings{kolar2011,
author = {J. Kolár and L. Lamel},
year = {2011},
month = {01},
pages = {833-836},
title = {On Development of Consistently Punctuated Speech Corpora}
}

@article{viera2005,
  title={Understanding interobserver agreement: the kappa statistic},
  author={A. J. Viera and J. M. Garrett and others},
  journal={Fam med},
  volume={37},
  number={5},
  pages={360--363},
  year={2005}
}

@INPROCEEDINGS{Bell2015,

  author={P. {Bell} and M. J. F. {Gales} and T. {Hain} and J. {Kilgour} and P. {Lanchantin} and X. {Liu} and A. {McParland} and S. {Renals} and O. {Saz} and M. {Wester} and P. C. {Woodland}},

  booktitle={2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)}, 

  title={The MGB challenge: Evaluating multi-genre broadcast media recognition}, 

  year={2015},

  volume={},

  number={},

  pages={687-693},

  doi={10.1109/ASRU.2015.7404863}}


@inproceedings{Klejch2016,  title     = "Punctuated Transcription of Multi-genre Broadcasts Using Acoustic and Lexical Approaches",  abstract  = "In this paper we investigate the punctuated transcription of multi-genre broadcast media. We examine four systems, three of which are based on lexical features, the fourth of which uses acoustic features by integrating punctuation into the speech recognition acoustic models. We also explore the combination of these component systems using voting and log-linear interpolation. We performed experiments on the English language MGB Challenge data, which comprises about 1,600h of BBC television recordings. Our results indicate that a lexical system, based on a neural machine translation approach is significantly better than other systems achieving an F-Measure of .626 on reference text, with a relative degradation of .19 on ASR output. Our analysis of the results in terms of specific punctuation indicated that using longer context improves the prediction of question marks and acoustic information improves prediction of exclamation marks. Finally, we show that even though the systems are complementary, their straightforward combination does not yield better F-measure",  author    = "O. Klejch and P. Bell and S. Renals",  year      = "2017",  month     = feb,  day       = "9",  doi       = "10.1109/SLT.2016.7846300",  language  = "English",  pages     = "433--440",  booktitle = "2016 IEEE Workshop on Spoken Language Technology",  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",  address   = "United States",  note      = "2016 IEEE Workshop on Spoken Language Technology, SLT 2016 ; Conference date: 13-12-2016 Through 16-12-2016",  url       = "https://www2.securecms.com/SLT2016//Default.asp", }

@inproceedings{levy2012,
  title={The effect of pitch, intensity and pause duration in punctuation detection},
  author={T. Levy and V. Silber-Varod and A. Moyal},
  booktitle={2012 IEEE 27th Convention of Electrical and Electronics Engineers in Israel},
  pages={1--4},
  year={2012},
  organization={IEEE}
}

@inproceedings{che2016,
  title={Punctuation prediction for unsegmented transcript based on word vector},
  author={X. Che and C. Wang and H. Yang and C. Meinel},
  booktitle={Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16)},
  pages={654--658},
  year={2016}
}

@INPROCEEDINGS{yi2019speech2vec,

  author={J. {Yi} and J. {Tao}},

  booktitle={ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 

  title={Self-attention Based Model for Punctuation Prediction Using Word and Speech Embeddings}, 

  year={2019},

  volume={},

  number={},

  pages={7270-7274},

  doi={10.1109/ICASSP.2019.8682260}}

@article{chung2018,
  title={Speech2vec: A sequence-to-sequence framework for learning word embeddings from speech},
  author={Y. A. Chung and J. Glass},
  journal={arXiv preprint arXiv:1803.08976},
  year={2018}
}

@inproceedings{LuNg2010,
author = {W. Lu and H. T. Ng},
title = {Better Punctuation Prediction with Dynamic Conditional Random Fields},
year = {2010},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {This paper focuses on the task of inserting punctuation symbols into transcribed conversational speech texts, without relying on prosodic cues. We investigate limitations associated with previous methods, and propose a novel approach based on dynamic conditional random fields. Different from previous work, our proposed approach is designed to jointly perform both sentence boundary and sentence type prediction, and punctuation prediction on speech utterances.We performed evaluations on a transcribed conversational speech domain consisting of both English and Chinese texts. Empirical results show that our method outperforms an approach based on linear-chain conditional random fields and other previous approaches.},
booktitle = {Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing},
pages = {177–186},
numpages = {10},
location = {Cambridge, Massachusetts},
series = {EMNLP '10}
}

  @article{wangngsim2012,
author = {X. Wang and H. Ng and K. Sim},
year = {2012},
month = {01},
pages = {},
title = {Dynamic Conditional Random Fields for Joint Sentence Boundary and Punctuation Prediction},
volume = {2},
journal = {13th Annual Conference of the International Speech Communication Association 2012, INTERSPEECH 2012}
}

@inproceedings{wangsimng2014,
    title = "Combining Punctuation and Disfluency Prediction: An Empirical Study",
    author = "X. Wang  and
      K. C. Sim  and
      H. T. Ng",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1013",
    doi = "10.3115/v1/D14-1013",
    pages = "121--130",
}

@inproceedings{Tilk2015,
author = {O. Tilk and T. Alumäe},
year = {2015},
month = {01},
pages = {},
title = {LSTM for Punctuation Restoration in Speech Transcripts}
}

@INPROCEEDINGS{Xu2016,

  author={K. {Xu} and L. {Xie} and K. {Yao}},

  booktitle={2016 10th International Symposium on Chinese Spoken Language Processing (ISCSLP)}, 

  title={Investigating LSTM for punctuation prediction}, 

  year={2016},

  volume={},

  number={},

  pages={1-5},

  doi={10.1109/ISCSLP.2016.7918492}}


@INPROCEEDINGS{Klejch2017,

  author={O. {Klejch} and P. {Bell} and S. {Renals}},

  booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 

  title={Sequence-to-sequence models for punctuated transcription combining lexical and acoustic features}, 

  year={2017},

  volume={},

  number={},

  pages={5700-5704},

  doi={10.1109/ICASSP.2017.7953248}}


@inproceedings{yitaowen2017,
author = {J. Yi and J. Tao and Z. Wen and Y. Li},
year = {2017},
month = {08},
pages = {2779-2783},
title = {Distilling Knowledge from an Ensemble of Models for Punctuation Prediction},
doi = {10.21437/Interspeech.2017-1079}
}

@misc{zelasko2018,
      title={Punctuation Prediction Model for Conversational Speech}, 
      author={P. Żelasko and P. Szymański and J. Mizgajski and A. Szymczak and Y. Carmiel and N. Dehak},
      year={2018},
      eprint={1807.00543},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{sunkara2020,
      title={Multimodal Semi-supervised Learning Framework for Punctuation Prediction in Conversational Speech}, 
      author={M. Sunkara and S. Ronanki and D. Bekal and S. Bodapati and K. Kirchhoff},
      year={2020},
      eprint={2008.00702},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}

@article{devlin2018,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={J. Devlin and M. W. Chang and K. Lee and K. Toutanova},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@misc{yi2020adversarial,
      title={Adversarial Transfer Learning for Punctuation Restoration}, 
      author={J. Yi and J. Tao and Y. Bai and Z. Tian and C. Fan},
      year={2020},
      eprint={2004.00248},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Makhoul2000,
author = {J. Makhoul and F. Kubala and R. Schwartz and R. Weischedel},
year = {2000},
month = {08},
pages = {},
title = {Performance Measures For Information Extraction},
journal = {Proceedings of DARPA Broadcast News Workshop}
}

@misc{vaswani2017,
      title={Attention Is All You Need}, 
      author={A. Vaswani and N. Shazeer and N. Parmar and J. Uszkoreit and L. Jones and A. N. Gomez and L. Kaiser and I. Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{lakew2018,
    title = "A Comparison of Transformer and Recurrent Neural Networks on Multilingual Neural Machine Translation",
    author = "S. M. Lakew  and
      M. Cettolo  and
      M. Federico",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/C18-1054",
    pages = "641--652",
    abstract = "Recently, neural machine translation (NMT) has been extended to multilinguality, that is to handle more than one translation direction with a single system. Multilingual NMT showed competitive performance against pure bilingual systems. Notably, in low-resource settings, it proved to work effectively and efficiently, thanks to shared representation space that is forced across languages and induces a sort of transfer-learning. Furthermore, multilingual NMT enables so-called zero-shot inference across language pairs never seen at training time. Despite the increasing interest in this framework, an in-depth analysis of what a multilingual NMT model is capable of and what it is not is still missing. Motivated by this, our work (i) provides a quantitative and comparative analysis of the translations produced by bilingual, multilingual and zero-shot systems; (ii) investigates the translation quality of two of the currently dominant neural architectures in MT, which are the Recurrent and the Transformer ones; and (iii) quantitatively explores how the closeness between languages influences the zero-shot translation. Our analysis leverages multiple professional post-edits of automatic translations by several different systems and focuses both on automatic standard metrics (BLEU and TER) and on widely used error categories, which are lexical, morphology, and word order errors.",
}

@article{roberta,
  author    = {Y. Liu and
               M. Ott and
               N. Goyal and
               J. Du and
               M. Joshi and
               D. Chen and
               O. Levy and
               M. Lewis and
               L. Zettlemoyer and
               V. Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692},
  archivePrefix = {arXiv},
  eprint    = {1907.11692},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{electra,
      title={ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators}, 
      author={K. Clark and M. T. Luong and Q. V. Le and C. D. Manning},
      year={2020},
      eprint={2003.10555},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@InProceedings{Sung_2018_CVPR,
author = {F. Sung and Y. Yang and L. Zhang and T. Xiang and P. H. S. Torr and T. M. Hospedales},
title = {Learning to Compare: Relation Network for Few-Shot Learning},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}

@misc{bakhturina_2019,
title={Punctuation — nemo 0.10.1 documentation},
url={https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en\\/v0.10.1/nlp/punctuation.html},
journal={docs.nvidia.com},
author={E. Bakhturina},
year={2019}
}

@article{nemo,
  author    = {O. Kuchaiev and
               J. Li and
               H. Nguyen and
               O. Hrinchuk and
               R. Leary and
               B. Ginsburg and
               S. Kriman and
               S. Beliaev and
               V. Lavrukhin and
               J. Cook and
               P. Castonguay and
               M. Popova and
               J. Huang and
               J. M. Cohen},
  title     = {NeMo: a toolkit for building {AI} applications using Neural Modules},
  journal   = {CoRR},
  volume    = {abs/1909.09577},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.09577},
  archivePrefix = {arXiv},
  eprint    = {1909.09577},
  timestamp = {Tue, 24 Sep 2019 11:33:51 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-09577.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{papineni-bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "K. Papineni  and
      S. Roukos  and
      T. Ward  and
      W.J. Zhu",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}

@article{palmer1995satz,
  title={SATZ-an adaptive sentence segmentation system},
  author={Palmer, David D},
  journal={arXiv preprint cmp-lg/9503019},
  year={1995}
}

@misc{li2020train,
      title={Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers}, 
      author={Z. Li and E. Wallace and S. Shen and K. Lin and K. Keutzer and D. Klein and J. E. Gonzalez},
      year={2020},
      eprint={2002.11794},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{fine_tuning,
  author    = {C. Alt and
               M. H{\"{u}}bner and
               L. Hennig},
  title     = {Fine-tuning Pre-Trained Transformer Language Models to Distantly Supervised
               Relation Extraction},
  journal   = {CoRR},
  volume    = {abs/1906.08646},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.08646},
  archivePrefix = {arXiv},
  eprint    = {1906.08646},
  timestamp = {Mon, 24 Jun 2019 17:28:45 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-08646.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{katharopoulos2020transformers,
      title={Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention}, 
      author={A. Katharopoulos and A. Vyas and N. Pappas and F. Fleuret},
      year={2020},
      eprint={2006.16236},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{time_aligned,
author = {M. Shugrina},
year = {2010},
month = {01},
pages = {198-206},
title = {Formatting Time-Aligned ASR Transcripts for Readability.}
}

@inproceedings{Ueffing2013,
  title={Improved models for automatic punctuation prediction for spoken and written text},
  author={B. Ueffing and M. Bisani and P. Vozila},
  booktitle={INTERSPEECH},
  year={2013}
}